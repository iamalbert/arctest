{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19da999a",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1f2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "from importlib import reload\n",
    "\n",
    "RAND_SEED=123\n",
    "\n",
    "with open(\"./job_postings_training_set.csv\") as fin:\n",
    "    reader = DictReader(fin)\n",
    "    \n",
    "    dataset = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdeccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sample = 17828, #(fraudulent=1)=862 (4.84%)\n"
     ]
    }
   ],
   "source": [
    "from utils import size_of\n",
    "    \n",
    "# print(len(dataset), dataset[0])\n",
    "n_samples = len(dataset)\n",
    "n_positive_samples = size_of(_ for _ in dataset if _['fraudulent'] == '1')\n",
    "print(f\"#sample = {n_samples}, #(fraudulent=1)={n_positive_samples} ({n_positive_samples/n_samples:.2%})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f46a905e",
   "metadata": {},
   "source": [
    "## Low-hanging fruit\n",
    "\n",
    "Let's see if any feature has high correlation to the predition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc50db25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names={'telecommuting__0': 'telecommuting: 0', 'telecommuting__1': 'telecommuting: 1', 'has_company_logo__0': 'has_company_logo: 0', 'has_company_logo__1': 'has_company_logo: 1', 'has_questions__0': 'has_questions: 0', 'has_questions__1': 'has_questions: 1', 'required_experience__0': 'required_experience: ', 'required_experience__1': 'required_experience: Associate', 'required_experience__2': 'required_experience: Director', 'required_experience__3': 'required_experience: Entry level', 'required_experience__4': 'required_experience: Executive', 'required_experience__5': 'required_experience: Internship', 'required_experience__6': 'required_experience: Mid-Senior level', 'required_experience__7': 'required_experience: Not Applicable', 'required_education__0': 'required_education: ', 'required_education__1': 'required_education: Associate Degree', 'required_education__2': \"required_education: Bachelor's Degree\", 'required_education__3': 'required_education: Certification', 'required_education__4': 'required_education: Doctorate', 'required_education__5': 'required_education: High School or equivalent', 'required_education__6': \"required_education: Master's Degree\", 'required_education__7': 'required_education: Professional', 'required_education__8': 'required_education: Some College Coursework Completed', 'required_education__9': 'required_education: Some High School Coursework', 'required_education__10': 'required_education: Unspecified', 'required_education__11': 'required_education: Vocational', 'required_education__12': 'required_education: Vocational - Degree', 'required_education__13': 'required_education: Vocational - HS Diploma', 'employment_type__0': 'employment_type: ', 'employment_type__1': 'employment_type: Contract', 'employment_type__2': 'employment_type: Full-time', 'employment_type__3': 'employment_type: Other', 'employment_type__4': 'employment_type: Part-time', 'employment_type__5': 'employment_type: Temporary', 'industry__0': 'industry: ', 'industry__1': 'industry: Accounting', 'industry__2': 'industry: Airlines/Aviation', 'industry__3': 'industry: Alternative Dispute Resolution', 'industry__4': 'industry: Animation', 'industry__5': 'industry: Apparel & Fashion', 'industry__6': 'industry: Architecture & Planning', 'industry__7': 'industry: Automotive', 'industry__8': 'industry: Aviation & Aerospace', 'industry__9': 'industry: Banking', 'industry__10': 'industry: Biotechnology', 'industry__11': 'industry: Broadcast Media', 'industry__12': 'industry: Building Materials', 'industry__13': 'industry: Business Supplies and Equipment', 'industry__14': 'industry: Capital Markets', 'industry__15': 'industry: Chemicals', 'industry__16': 'industry: Civic & Social Organization', 'industry__17': 'industry: Civil Engineering', 'industry__18': 'industry: Commercial Real Estate', 'industry__19': 'industry: Computer & Network Security', 'industry__20': 'industry: Computer Games', 'industry__21': 'industry: Computer Hardware', 'industry__22': 'industry: Computer Networking', 'industry__23': 'industry: Computer Software', 'industry__24': 'industry: Construction', 'industry__25': 'industry: Consumer Electronics', 'industry__26': 'industry: Consumer Goods', 'industry__27': 'industry: Consumer Services', 'industry__28': 'industry: Cosmetics', 'industry__29': 'industry: Defense & Space', 'industry__30': 'industry: Design', 'industry__31': 'industry: E-Learning', 'industry__32': 'industry: Education Management', 'industry__33': 'industry: Electrical/Electronic Manufacturing', 'industry__34': 'industry: Entertainment', 'industry__35': 'industry: Environmental Services', 'industry__36': 'industry: Events Services', 'industry__37': 'industry: Executive Office', 'industry__38': 'industry: Facilities Services', 'industry__39': 'industry: Farming', 'industry__40': 'industry: Financial Services', 'industry__41': 'industry: Fishery', 'industry__42': 'industry: Food & Beverages', 'industry__43': 'industry: Food Production', 'industry__44': 'industry: Fund-Raising', 'industry__45': 'industry: Furniture', 'industry__46': 'industry: Gambling & Casinos', 'industry__47': 'industry: Government Administration', 'industry__48': 'industry: Government Relations', 'industry__49': 'industry: Graphic Design', 'industry__50': 'industry: Health, Wellness and Fitness', 'industry__51': 'industry: Higher Education', 'industry__52': 'industry: Hospital & Health Care', 'industry__53': 'industry: Hospitality', 'industry__54': 'industry: Human Resources', 'industry__55': 'industry: Import and Export', 'industry__56': 'industry: Individual & Family Services', 'industry__57': 'industry: Industrial Automation', 'industry__58': 'industry: Information Services', 'industry__59': 'industry: Information Technology and Services', 'industry__60': 'industry: Insurance', 'industry__61': 'industry: International Trade and Development', 'industry__62': 'industry: Internet', 'industry__63': 'industry: Investment Banking', 'industry__64': 'industry: Investment Management', 'industry__65': 'industry: Law Enforcement', 'industry__66': 'industry: Law Practice', 'industry__67': 'industry: Legal Services', 'industry__68': 'industry: Leisure, Travel & Tourism', 'industry__69': 'industry: Libraries', 'industry__70': 'industry: Logistics and Supply Chain', 'industry__71': 'industry: Luxury Goods & Jewelry', 'industry__72': 'industry: Machinery', 'industry__73': 'industry: Management Consulting', 'industry__74': 'industry: Maritime', 'industry__75': 'industry: Market Research', 'industry__76': 'industry: Marketing and Advertising', 'industry__77': 'industry: Mechanical or Industrial Engineering', 'industry__78': 'industry: Media Production', 'industry__79': 'industry: Medical Devices', 'industry__80': 'industry: Medical Practice', 'industry__81': 'industry: Mental Health Care', 'industry__82': 'industry: Military', 'industry__83': 'industry: Mining & Metals', 'industry__84': 'industry: Motion Pictures and Film', 'industry__85': 'industry: Museums and Institutions', 'industry__86': 'industry: Music', 'industry__87': 'industry: Nanotechnology', 'industry__88': 'industry: Nonprofit Organization Management', 'industry__89': 'industry: Oil & Energy', 'industry__90': 'industry: Online Media', 'industry__91': 'industry: Outsourcing/Offshoring', 'industry__92': 'industry: Package/Freight Delivery', 'industry__93': 'industry: Packaging and Containers', 'industry__94': 'industry: Performing Arts', 'industry__95': 'industry: Pharmaceuticals', 'industry__96': 'industry: Philanthropy', 'industry__97': 'industry: Photography', 'industry__98': 'industry: Plastics', 'industry__99': 'industry: Primary/Secondary Education', 'industry__100': 'industry: Printing', 'industry__101': 'industry: Professional Training & Coaching', 'industry__102': 'industry: Program Development', 'industry__103': 'industry: Public Policy', 'industry__104': 'industry: Public Relations and Communications', 'industry__105': 'industry: Public Safety', 'industry__106': 'industry: Publishing', 'industry__107': 'industry: Ranching', 'industry__108': 'industry: Real Estate', 'industry__109': 'industry: Religious Institutions', 'industry__110': 'industry: Renewables & Environment', 'industry__111': 'industry: Research', 'industry__112': 'industry: Restaurants', 'industry__113': 'industry: Retail', 'industry__114': 'industry: Security and Investigations', 'industry__115': 'industry: Semiconductors', 'industry__116': 'industry: Shipbuilding', 'industry__117': 'industry: Sporting Goods', 'industry__118': 'industry: Sports', 'industry__119': 'industry: Staffing and Recruiting', 'industry__120': 'industry: Telecommunications', 'industry__121': 'industry: Textiles', 'industry__122': 'industry: Translation and Localization', 'industry__123': 'industry: Transportation/Trucking/Railroad', 'industry__124': 'industry: Utilities', 'industry__125': 'industry: Venture Capital & Private Equity', 'industry__126': 'industry: Veterinary', 'industry__127': 'industry: Warehousing', 'industry__128': 'industry: Wholesale', 'industry__129': 'industry: Wine and Spirits', 'industry__130': 'industry: Wireless', 'industry__131': 'industry: Writing and Editing', 'function__0': 'function: ', 'function__1': 'function: Accounting/Auditing', 'function__2': 'function: Administrative', 'function__3': 'function: Advertising', 'function__4': 'function: Art/Creative', 'function__5': 'function: Business Analyst', 'function__6': 'function: Business Development', 'function__7': 'function: Consulting', 'function__8': 'function: Customer Service', 'function__9': 'function: Data Analyst', 'function__10': 'function: Design', 'function__11': 'function: Distribution', 'function__12': 'function: Education', 'function__13': 'function: Engineering', 'function__14': 'function: Finance', 'function__15': 'function: Financial Analyst', 'function__16': 'function: General Business', 'function__17': 'function: Health Care Provider', 'function__18': 'function: Human Resources', 'function__19': 'function: Information Technology', 'function__20': 'function: Legal', 'function__21': 'function: Management', 'function__22': 'function: Manufacturing', 'function__23': 'function: Marketing', 'function__24': 'function: Other', 'function__25': 'function: Product Management', 'function__26': 'function: Production', 'function__27': 'function: Project Management', 'function__28': 'function: Public Relations', 'function__29': 'function: Purchasing', 'function__30': 'function: Quality Assurance', 'function__31': 'function: Research', 'function__32': 'function: Sales', 'function__33': 'function: Science', 'function__34': 'function: Strategy/Planning', 'function__35': 'function: Supply Chain', 'function__36': 'function: Training', 'function__37': 'function: Writing/Editing'}\n",
      "X.shape=(17828, 204), Y.shape=(17828, 1)\n",
      "X_train.shape=(12479, 204) y_train.shape=(12479, 1) | X_test.shape=(5349, 204) y_test.shape=(5349, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "\n",
    "interested_fields =  ['telecommuting', 'has_company_logo', 'has_questions', 'required_experience', 'required_education', 'employment_type', 'industry', 'function']\n",
    "\n",
    "\n",
    "input_enc = OneHotEncoder()\n",
    "\n",
    "categorical_dataset: list[list[str]] = [\n",
    "    [d[fn] for fn in interested_fields]\n",
    "    for d in dataset \n",
    "]\n",
    "\n",
    "X = input_enc.fit_transform(categorical_dataset).toarray()\n",
    "\n",
    "\n",
    "feature_names:dict[str, str] = dict(\n",
    "    chain.from_iterable(\n",
    "        ((f\"{fn}__{index}\", f\"{fn}: {cat}\") for index,cat in enumerate(category)) for fn, category in zip(interested_fields, input_enc.categories_)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"{feature_names=}\")\n",
    "\n",
    "\n",
    "target_enc = OrdinalEncoder(categories=[['0','1']])\n",
    "target_set: list[list[str]] = [   [d['fraudulent']] for d in dataset ]\n",
    "\n",
    "Y = target_enc.fit_transform(target_set)\n",
    "\n",
    "print(f\"{X.shape=}, {Y.shape=}\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=RAND_SEED)\n",
    "\n",
    "print(f\"{X_train.shape=} {y_train.shape=} | {X_test.shape=} {y_test.shape=}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed2b6c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(acc=0.9553, f1=0.2866, precision=0.7273, recall=0.1784) LogisticRegressionCV(cv=3, random_state=123, solver='newton-cholesky')\n",
      "Result(acc=0.9553, f1=0.2866, precision=0.7273, recall=0.1784) LogisticRegressionCV(cv=3, random_state=123, solver='liblinear')\n",
      "Result(acc=0.8048, f1=0.2927, precision=0.1790, recall=0.8030) LogisticRegressionCV(class_weight='balanced', cv=3, random_state=123,\n",
      "                     solver='liblinear')\n",
      "Result(acc=0.6250, f1=0.1970, precision=0.1104, recall=0.9145) LogisticRegressionCV(class_weight={0: 1, 1: 100}, cv=3, random_state=123,\n",
      "                     solver='liblinear')\n",
      "Result(acc=0.9729, f1=0.6742, precision=0.8523, recall=0.5576) LGBMClassifier(random_state=123)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "\n",
    "def solve(reg, **kwargs):\n",
    "    reg.fit(X_train, column_or_1d(y_train), **kwargs)\n",
    "\n",
    "    y_pred = reg.predict(X_test)\n",
    "    res = utils.acc_and_f1(y_test, y_pred)\n",
    "\n",
    "    print(f\"{res!s} {reg!s}\")\n",
    "\n",
    "    return reg\n",
    "\n",
    "\n",
    "\n",
    "solve(LogisticRegressionCV(cv=3, random_state=RAND_SEED, refit=True, solver='newton-cholesky'))\n",
    "solve(LogisticRegressionCV(cv=3, random_state=RAND_SEED, refit=True, solver='liblinear'))\n",
    "best_lr = solve(LogisticRegressionCV(cv=3, random_state=RAND_SEED, refit=True, solver='liblinear', class_weight='balanced'))\n",
    "solve(LogisticRegressionCV(cv=3, random_state=RAND_SEED, refit=True, solver='liblinear', class_weight={0:1, 1:100}))\n",
    "best = solve(LGBMClassifier(random_state=RAND_SEED), feature_name=list(feature_names.keys()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ed1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 required_education: High School or equivalent\n",
      "102 has_questions: 0\n",
      "101 has_company_logo: 0\n",
      "98 required_experience: Entry level\n",
      "91 required_education: \n",
      "86 employment_type: Full-time\n",
      "84 industry: Oil & Energy\n",
      "71 required_experience: Mid-Senior level\n",
      "67 required_experience: \n",
      "64 required_education: Bachelor's Degree\n",
      "62 function: Engineering\n",
      "59 required_experience: Not Applicable\n",
      "59 employment_type: Part-time\n",
      "55 function: Administrative\n",
      "53 function: Sales\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "feature_importances = sorted(zip(best.feature_importances_, best.feature_name_), reverse=True)\n",
    "\n",
    "for importance, name in feature_importances[:15]:\n",
    "    print( importance, feature_names[name], )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d34a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3360 industry: Oil & Energy\n",
      "5.2076 industry: Media Production\n",
      "4.7393 industry: Animation\n",
      "4.6438 industry: Computer Networking\n",
      "4.2183 industry: Hospitality\n",
      "4.1158 industry: Design\n",
      "4.0570 industry: Leisure, Travel & Tourism\n",
      "4.0001 industry: Accounting\n",
      "3.9713 function: Distribution\n",
      "3.7581 industry: Computer & Network Security\n",
      "3.5490 industry: Biotechnology\n",
      "3.5076 industry: Health, Wellness and Fitness\n",
      "3.5034 industry: Hospital & Health Care\n",
      "3.4176 function: Business Development\n",
      "3.3749 industry: Information Services\n"
     ]
    }
   ],
   "source": [
    "coef = best_lr.coef_.ravel()\n",
    "best_feature_index = coef.ravel().argsort()[::-1]\n",
    "\n",
    "\n",
    "names= list(feature_names.values())\n",
    "for index in best_feature_index[:15]:\n",
    "    print(f\"{coef[index]:.04f} {names[index]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3123afcf",
   "metadata": {},
   "source": [
    "# Deep learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7910b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=204, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(RAND_SEED)\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = ( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hsize=512):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(204, hsize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hsize, hsize),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hsize, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train.ravel())), batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test.ravel())), batch_size=640000000)\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(X, y)\n",
    "\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "from torcheval.metrics.functional import (binary_f1_score, binary_accuracy, binary_precision, binary_recall)\n",
    "from utils import Result \n",
    "\n",
    "def test(dataloader, model, loss_fn, name=\"\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            pred_max_indices = pred.argmax(1)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # correct += (pred_max_indices == y).type(torch.float).sum().item()\n",
    "\n",
    "            result = Result(\n",
    "                binary_accuracy(pred_max_indices, y).item(),\n",
    "                binary_f1_score(pred_max_indices, y).item(),\n",
    "                binary_precision(pred_max_indices, y).item(),\n",
    "                binary_recall(pred_max_indices, y).item(),\n",
    "            )\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "\n",
    "    print(f\"Test Error: Avg loss {test_loss:>8f}\\n{result} {name}\\n\")\n",
    "\n",
    "\n",
    "model = NeuralNetwork(256).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def run(loss_fn, epochs=7, name=\"\"):\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(test_dataloader, model, loss_fn, name or repr(loss_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2705060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "loss: 0.041175  [12285/12479]\n",
      "Test Error: Avg loss 0.093124\n",
      "Result(acc=0.9748, f1=0.6966, precision=0.8807, recall=0.5762) CrossEntropyLoss\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "loss: 0.031357  [12285/12479]\n",
      "Test Error: Avg loss 0.103372\n",
      "Result(acc=0.9746, f1=0.6881, precision=0.8982, recall=0.5576) CrossEntropyLoss\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "loss: 0.047696  [12285/12479]\n",
      "Test Error: Avg loss 0.097866\n",
      "Result(acc=0.9731, f1=0.6786, precision=0.8492, recall=0.5651) CrossEntropyLoss\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "loss: 0.114220  [12285/12479]\n",
      "Test Error: Avg loss 0.107139\n",
      "Result(acc=0.9727, f1=0.6894, precision=0.8060, recall=0.6022) CrossEntropyLoss\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "loss: 0.044064  [12285/12479]\n",
      "Test Error: Avg loss 0.102599\n",
      "Result(acc=0.9744, f1=0.6949, precision=0.8667, recall=0.5799) CrossEntropyLoss\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "loss: 0.032117  [12285/12479]\n",
      "Test Error: Avg loss 0.100742\n",
      "Result(acc=0.9736, f1=0.6928, precision=0.8368, recall=0.5911) CrossEntropyLoss\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "loss: 0.013574  [12285/12479]\n",
      "Test Error: Avg loss 0.102879\n",
      "Result(acc=0.9748, f1=0.7020, precision=0.8641, recall=0.5911) CrossEntropyLoss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(nn.CrossEntropyLoss(), name = \"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3916e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.267659  [12285/12479]\n",
      "Test Error: Avg loss 0.366129\n",
      "Result(acc=0.9123, f1=0.4874, precision=0.3452, recall=0.8290) CrossEntropyLoss(weighed)\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.107637  [12285/12479]\n",
      "Test Error: Avg loss 0.430486\n",
      "Result(acc=0.9303, f1=0.5423, precision=0.4048, recall=0.8216) CrossEntropyLoss(weighed)\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.933977  [12285/12479]\n",
      "Test Error: Avg loss 0.606592\n",
      "Result(acc=0.9518, f1=0.6055, precision=0.5143, recall=0.7361) CrossEntropyLoss(weighed)\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.288457  [12285/12479]\n",
      "Test Error: Avg loss 0.445586\n",
      "Result(acc=0.8815, f1=0.4215, precision=0.2793, recall=0.8587) CrossEntropyLoss(weighed)\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.074084  [12285/12479]\n",
      "Test Error: Avg loss 0.506686\n",
      "Result(acc=0.9306, f1=0.5437, precision=0.4062, recall=0.8216) CrossEntropyLoss(weighed)\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.182013  [12285/12479]\n",
      "Test Error: Avg loss 0.575621\n",
      "Result(acc=0.8994, f1=0.4555, precision=0.3129, recall=0.8364) CrossEntropyLoss(weighed)\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.091245  [12285/12479]\n",
      "Test Error: Avg loss 0.613123\n",
      "Result(acc=0.9265, f1=0.5293, precision=0.3905, recall=0.8216) CrossEntropyLoss(weighed)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(nn.CrossEntropyLoss(weight=torch.Tensor([1, 20]).to(device)), name=\"CrossEntropyLoss(weighed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08eedf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "loss: 0.169387  [12285/12479]\n",
      "Test Error: Avg loss 0.180617\n",
      "Result(acc=0.9740, f1=0.7036, precision=0.8250, recall=0.6134) CrossEntropyLoss(label_smoothing)\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "loss: 0.149342  [12285/12479]\n",
      "Test Error: Avg loss 0.183062\n",
      "Result(acc=0.9733, f1=0.6772, precision=0.8621, recall=0.5576) CrossEntropyLoss(label_smoothing)\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "loss: 0.178648  [12285/12479]\n",
      "Test Error: Avg loss 0.182981\n",
      "Result(acc=0.9723, f1=0.6636, precision=0.8538, recall=0.5428) CrossEntropyLoss(label_smoothing)\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "loss: 0.226543  [12285/12479]\n",
      "Test Error: Avg loss 0.178986\n",
      "Result(acc=0.9740, f1=0.6918, precision=0.8571, recall=0.5799) CrossEntropyLoss(label_smoothing)\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "loss: 0.145692  [12285/12479]\n",
      "Test Error: Avg loss 0.180950\n",
      "Result(acc=0.9738, f1=0.6943, precision=0.8413, recall=0.5911) CrossEntropyLoss(label_smoothing)\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "loss: 0.173156  [12285/12479]\n",
      "Test Error: Avg loss 0.184273\n",
      "Result(acc=0.9710, f1=0.6764, precision=0.7714, recall=0.6022) CrossEntropyLoss(label_smoothing)\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "loss: 0.141124  [12285/12479]\n",
      "Test Error: Avg loss 0.179947\n",
      "Result(acc=0.9729, f1=0.6827, precision=0.8298, recall=0.5799) CrossEntropyLoss(label_smoothing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(nn.CrossEntropyLoss(label_smoothing =0.05), name=\"CrossEntropyLoss(label_smoothing)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
